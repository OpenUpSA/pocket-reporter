{
  "body": "Source Link: <https://app.pocketreporter.co.za/documents/Media_and_Surveys.pdf>\n\n# Toolkit\n\nTOOLKIT NO. 1 PRODUCED BY IDASA’S WORD_ON_THE_STREET_ ([www.wordonthestreet.org.za](www.wordonthestreet.org.za))\n\nMedia organisations, governments and businesses regularly make use of public opinion surveys as a way of finding out how members of\n the public feel about all sorts of issues.\n\nOpinion surveys are widely used in the run-up\n to elections, as a way of trying to predict the outcome or measure\n the success of the parties’ campaigns. But public opinion surveys\n increasingly play a role in-between elections too.\n\nOpinion surveys can be a rich and useful resource for journalists,\n but if journalists wish to benefit from this tool, they need to understand it. They need to know how surveys work, how to evaluate\n them, and how to interpret them. \n\n**This toolkit provides\n some basic guidelines to\n assist journalists in\n knowing how surveys\n work, how to evaluate\n them and how to\n interpret them.**\n\n- - -\n\n## **Introduction**\n\nRegular elections are seen as the most\n important way for citizens in a democracy to make their views known. But elections have limitations. For one thing, they\n are rather infrequent – most are held only\n every 4 or 5 years. Secondly, when elections do come around, not everybody participates – in some countries less than half of the population turns\n out to vote. Finally, elections are rather blunt instruments – citizens\n have just one vote per ballot paper. That is very useful when it\n comes to making a single choice, such as who should be the ruling\n party, but it’s not a useful way for citizens to reveal a range of preferences.\n\nIf they are to be responsive to citizens’ preferences, then in\n between elections, government, business, and social institutions\n need a timely and accurate flow of information on citizens’ preferences, needs and behaviour. So, while public opinion surveys are\n often done in the run-up to elections, they increasingly play a role\n in-between elections too. Media organisations, governments and\n businesses regularly make use of public opinion surveys as a way\n of measuring public sentiment.\n\nIn many parts of the world, survey researchers and news media\n are viewed as natural allies. If news media see themselves as “watchdogs” or “guardians of the public interest,” then opinion\n surveys are far better guides to what the public want and think than\n the typical “man on the street” or “voxpop” interview. In North\n America, Western Europe and Japan, many large news organisations maintain in-house survey departments, and regularly conduct\n surveys or polls to find out how people feel about the key issues of\n the day. At election time and during other critical events, such surveys can play a powerful role in focusing attention on ordinary\n people’s viewpoints and priorities. Often, these turn out to be\n rather different from those of the elite groups who usually hog the\n headlines. \n\nSurveys are useful for many reasons.\n In contrast to elections, they can be done\n at any time, and as frequently as\n resources permit. They can count voters\n and non-voters equally, or compare them.\n They can help separate values, evaluations, policy preferences from the vote\n and they can distinguish among several\n preferences or evaluations.\n So, surveys can be both a benefit and\n threat to governments, parties and organisations. For those who are truly interested, surveys let us know what citizens are thinking and what they\n want. For those who are interested in “leading” or “educating” the\n public, surveys provide a key source of information (“intelligence”). For those who are involved in policy debate, public opinion can be a tool (if the people support you) or a threat (if they\n oppose you). That is why, in most modern democracies, opinion\n surveys are increasingly viewed as one of the most important communication links between governments and the governed. \n\n**What is a public opinion survey?**\n\nA survey is a method of gathering information from a sample of\n individuals within a particular group or population. This information is then used to draw conclusions about the entire group or population.\n\nThe idea of a ‘sample’ is crucial. A survey is not like a census.\n In a census, all members of the population are studied. In a survey,\n a sample is selected. They key to a scientific survey is that the\n chance (or probability) or every unit (or person) in the population\n being selected for the sample, must be known. In a purely random\n sample, each unit in the population must have the same probability of being selected.\n\nA survey is also different from a focus group. In a focus group,\n several people are encouraged by a group leader to voice their\n reactions to various issues. Focus groups may provide interesting\n insight for certain purposes, but they cannot be used to draw inferences about the larger population, because they do not make use of\n systematic, random sampling.\n\nA public opinion survey focuses on individuals – the individual\n is the unit of measurement. Thus, it is different from a household\n survey, which has the entire household as its unit of measurement.\n\nA public opinion survey gathers information about just that:\n opinions – people’s preferences or evaluations. It does not look at\n behaviour, such as whether people are looking for jobs, or how\n people spend their money\n\n**Are surveys accurate?**\n\nThe results of a survey can be used to draw inferences about an\n entire population for a number of reasons. Firstly, samples are scientifically chosen, so that each person in the population will have\n an equal and known chance of being selected. Secondly, information is collected in a standardised way, so that every individual is\n asked the same questions in the same way.\n\nSurveys that follow a sound methodology can thus provide a\n very accurate insight into people’s opinions. But surveys also have\n their limitations. For one thing, they should be seen as a ‘snapshot’\n of people’s views, at a particular time. As circumstances change, or\n new information is made public, people’s opinions may also\n change. This is particularly true of polls during election campaigns, where survey results might change from week to week, or\n even day to day.\n\nSeveral factors can impact on the results of a survey. One important factor is sampling error. This means that the results from the\n sample group differ from the results that would be found in the\n population as a whole, because the sample doesn’t accurately\n reflect the broader population. One common source of sampling\n error is the way in which surveys are conducted. For example, a\n survey that uses only telephonic interviews to gather people’s\n responses cannot claim to be representative of the whole South\n African population, as millions of South Africans do not have\n phones and they would have been excluded from participating.\n\nThere are also many kinds of non-sampling error that can affect\n the accuracy of a survey. Question Design and question order are\n very important, as the way in which questions are worded and\n organised can have a dramatic impact on the survey results.\n Another problem with surveys is that people may not be comfortable revealing their real opinions. This could happen in cases\n where there are large cultural, economic and other differences\n between the interviewers and interviewees. Language is another\n potential source of error. If surveys are conducted in a language\n other than people’s home language, there is a danger that respondents will misunderstand the questions, or that their answers will\n be misunderstood. On the other hand, if a survey questionnaire is\n translated into people’s home languages, there is the danger that the translated questions may not all be asking exactly the same\n thing. Thus, the answers from different language groups may not\n be able to be compared with one another. Strict measures and\n checks have to be put into place to ensure that translated questionnaires are all uniform.\n\n“Fake” surveys: It is important to beware of surveys that are not\n conducted scientifically. This includes any survey in which the\n respondents select themselves. Examples are polls on the Internet,\n where visitors to a web-site are asked to ‘vote’ on one issue or\n another, or polls conducted by newspapers or radio and television\n stations, where listeners and viewers are asked to call or SMS to\n indicate their preferences. In all these cases, it is the respondents\n themselves who decide to participate, and the group of respondents\n is thus not likely to be representative of the broader population in\n general. Such polls may be fun and entertaining, but they cannot\n claim to give us insight into the general, ‘public’opinion.\n\nWhile there are many possible sources of error, surveys that are\n conducted according to sound scientific methods can provide highly accurate insights into public opinion.\n\n## **Key questions every journalist should ask**\n\n**Who conducted the poll?**\n\nYou need to know this, so you can get all the information from\n them that will assist you to evaluate the survey and to follow up on\n stories. You should also be able to find out whether the organisation conducting the poll is a reputable one, with a good track\n record.\n\n**Who commissioned it, and why?**\n\nThe answer to this question will help you evaluate the validity of\n the survey. For example, if the survey was commissioned by a special interest group, a political party or a private corporation, you\n should be very careful, since such bodies usually commission\n research in order to achieve a specific goal, and this might have led\n to bias in the sampling, the timing, the phrasing of questions, or the\n interpretation of data.\n\n**Who does the survey cover?**\n\n(What universe did the survey cover?)\n\nIs the survey claiming to survey the views of all citizens of your\n country, or only a specific group?\n\n**What was the sampling method?**\n\n(How were the interviewees selected?)\n\nYou need to know whether the researcher used a specific scientific method for picking respondents, or whether people volunteered\n to participate (a sure sign of an unscientific survey).\n\nYou need to know whether the sampling was done in order to\n reflect the diversity of the population. (For example, if 20% of the\n country’s population lives in Province A, then 20% of the individuals in the sample must be from Province A. If 40% of the country\n lives in rural areas, then 40% of people in the sample must be from\n rural areas. If the population is 48% male and 52% female, then\n this must also be true of the sample – all depending on what the\n survey is claiming to measure.)\n\n**What was the sampling error? (overall, and for any subgroups)**\n\nThis is important. For example, if the survey says that 52% of respondents were in favour of a new law with 48% against it, and\n the margin of error is +/- 5 percentage points, you cannot say “most\n people favour the new law”. The margin of error means that with\n the figures so close, it’s impossible to say.\n\n**How was the data collected?**\n\nDid the researcher send out questionnaires by mail, use telephonic\n interviews, or interview people in person?\n\nMail surveys take time and thus risk some responses being out\n of date. They also have high levels of non-respondents.\n\nTelephonic surveys can only claim to represent people living in\n households with telephones. Since cell phone numbers are not listed, it is generally only people with landlines who are interviewed.\n This generally means that poorer people and rural dwellers are\n excluded, and in countries like South Africa it also means the survey will have a large racial bias.\n\nIf the interviews were done in person, were they done in people’s homes, or in shopping malls or on pavements? If they were\n done in places other than homes, the results should be viewed\n sceptically – since the sampling won’t have been random, representative and unbiased (for example: if the survey was done in a\n shopping mall it would only include people with money to spend,\n or the transport to get to the shops).\n\nHow many non-responses were there, and what steps were taken\n to minimize non-responses? No survey gets a 100% response rate.\n There are many reasons for this. Some people aren’t home, or\n refuse to participate. If the non-response rate is large, or disproportionately large among a specific sub-group, that’s a problem.\n For example, if the survey was conducted during the day, large\n numbers of employed people may have been excluded.\n\n**When was the survey done?**\n\nEvents can have a dramatic impact on results. For example: if, in\n 1998, a survey had set out to ask citizens of Nairobi about their\n feelings of physical safety, the answers would have been dramatically different, depending on whether the survey had been held\n before or after August 7 – the day the US embassy in Nairobi was\n bombed.\n\n**What questions were asked, and in what order?**\n\nThe phrasing of questions can have a dramatic impact on the\n results. Test your own reaction to any specific question – does it\n seem fair and unbiased? Does it offer a balanced set of choices?\n Would most people clearly understand the question?\n\nThe order in which questions are asked can influence the results.\n For example, if people are asked to identify the biggest problems\n facing their country and are then asked to evaluate the government’s performance immediately afterwards, that could lead to a\n more negative rating of the government than if they had been asked\n to rate the government earlier on.\n\n**Reporting on public opinion surveys**\n\nThere are several different ways to report on public opinion surveys. These include using the survey as a news report itself; using\n the survey for background material; using the survey to provide\n market research; and using the survey as a hook, to develop a story.\n\n**Public opinion as news**\n\nPolls have become news in themselves. During elections, a new survey result can provide the day’s headline: “X leading by far,\n says new poll”. But this is true not only for political campaigns.\n Journalists often use interesting surveys as stories in themselves.\n Polls can be about trivial issues (such as what people feel about a\n film star’s new marriage), or more serious issues such as citizen’s\n opinions about the government’s performance.\n\n**Public opinion as background material**\n\nBesides being news in themselves, surveys provide crucial background to the news by letting news media know how people view\n politics and government, and by helping journalists identify key\n social issues that they should be focusing on in their stories. Some\n examples of the kind of background information that surveys can\n\nprovide are:\n\n* Which problems do people want government to focus on?\n* How well do people feel government is doing its job?\n* How much corruption do people see in government?\n* Who is likely to win the next election?\n\n**Public opinion as market research**\n\nSurveys can be useful for the news media, not to provide stories,\n but to provide them with market research about how the public\n view them. For example:\n\n* How often do people use news media?\n* How much do they trust news media?\n\n**Public opinion as a story “hook”**\n\nMost surveys offer a number of opportunities for stories, and any\n given survey result can be looked at from a number of angles.\n\nAs a first take on a newly released set of survey results, you may\n simply want to report on the facts and figures. For example, the\n 2004 Afrobarometer study in South Africa contains a number of\n interesting figures on people’s opinions about crime and security.\n The study found that only 31% of South Africans name crime as\n one of the top three problems facing the country that the government should address. This level is at its lowest since 1995. Since\n reports and debates about crime have been highly prominent in the\n South African media since 1994, these figures are newsworthy in\n themselves.\n\nIf a survey is really important, politically or socially, you will\n probably want to do more than simply reflect the facts and figures.\n You may want to build a bigger story, probe the meaning of the\n findings and go into some depth on the issues the study raises.\n There are a number of options available to journalists, such as\n obtaining responses from politicians, getting some reactions from\n ordinary people on the street, and finding one or two experts in the\n field to provide some interpretation of the findings and their implications. Such an approach would work well with the example of\n attitudes to crime, given above. Since this is such a big issue in\n South Africa, it would be well worth getting some politicians’ reactions, and asking a few experts to comment on the study, and significance, and speculate about the reasons for the findings.\n\nSurveys often provide good topics for radio and TV discussion\n programmes. Again, this would be true for the crime example. The\n topic lends itself well to a panel discussion or a call in show. In the\n case of a call-in show, the programme host could invite one of the\n researchers into the studio to present the key findings, and then ask\n viewers or listeners what they think.\n\nWhen you are exploring the political implication of survey\n results, bear in mind that any given survey usually contains many\n results & stories. For example, crime is just one of the areas cov-ered by the Afrobarometer survey in South Africa. The same survey also looked into perceptions on government corruption, AIDS,\n people’s experiences with the public service, and people’s experiences of poverty. This offers a wealth of potential story material,\n and a single journalist could spend months just working on story\n ideas coming out of this survey.\n\nIn addition to this, any given result has multiple angles, and survey results are likely to differ across race, gender, age, geographical region, and other categories. In the example of the questions on\n crime in South Africa, the Afrobarometer found sharp differences\n in perceptions according to race. It found that while blacks tended\n to view the situation as getting better rather than worse, the opposite was true for whites, who were more likely to see things as getting worse rather than better. This was despite the fact that of all\n the groups, blacks were most likely to have felt insecure in their\n homes, or to have had a family member attacked. \n\n**Key definitions**\n\n• _Sampling_: This is the process of selecting a small section of the\n overall population. Research will be done with the sample\n group, in order to draw conclusions about the population as a\n whole.\n\n• _Sampling error_: Any mistakes or inaccuracies in the sampling so\n that the sample is not an accurate reflection of the broader population.\n\n• _Random sampling_: Each member of the population being studied has the same probability of being selected to be part of the\n sample group.\n\n• _Universe/population_: The larger group of people about whom the researchers want to be able to draw conclusions or generalise\n their findings. For example: All adult South Africans, residents\n of Lilongwe, or Kenyan teenagers aged between 12 and 19.\n\n• _Focus groups_: Focus groups are not surveys or polls, but are indepth interviews with a small, carefully selected group of people\n on a particular topic. They are qualitative rather than quantitative. Focus groups are becoming more and more popular as a\n way of learning about opinions and attitudes. They help you get\n an in-depth sense of what makes people tick and of why people\n think the way they do. They can’t be used to draw scientifically\n valid conclusions about the broader population in the way that\n surveys can. However, focus groups can be used before surveys\n take place, to pre-test ideas. \n\n**For more information**\n\nBackground information on understanding surveys or polls:\n\nCovering Polls: A Handbook for Journalists (Published by\n the Media Studies Center:\n\n[http://www.aceproject.org/main/samples/me/mex29.pdf\n](http://www.aceproject.org/main/samples/me/mex29.pdf)\n\nA Media Guide to Survey Research (Published by the\n World Association for Public Opinion Research:\n\n[http://www.unl.edu/WAPOR/journalists.doc\n](http://www.unl.edu/WAPOR/journalists.doc)\n\nWhat is a Survey (Written by Fritz Scheuren:\n\n[http://www.whatisasurvey.info/\n](http://www.whatisasurvey.info/)\n\nExamples of surveys/organisations that undertake surveys:\n\nAfrobarometer: [http://www.afrobarometer.org\n](http://www.afrobarometer.org)\n\nSouth African Advertising Research Foundation:\n\n<http://www.saarf.co.za>\n\nProduced by IDASA’s All Media Group as part of its word_on_the_street_ project, which is funded by the Embassy of Finland\n\n([www.wordonthestreet.org.za](www.wordonthestreet.org.za))\n\nFor more information contact the All Media Group at\n\n6 Spin Street\n Cape Town\n 8001\n\nph: (021) 4675600\n\nfax: (021) 4653337\n\nemail: [brett@idasact.org.za\n](brett@idasact.org.za)\n\n[www.idasa.org.za\n](www.idasa.org.za)\n\nFor conditions of use, please see our website: [www.wordonthestreet.org.za](www.wordonthestreet.org.za)\n\n# i_d_asa",
  "spa": {},
  "nso": {},
  "afr": {},
  "tsn": {},
  "zul": {},
  "por": {},
  "sot": {},
  "title": "Reporting on public opinion surveys",
  "type": "resources",
  "xho": {}
}